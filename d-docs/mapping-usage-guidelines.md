# Mapping GitHub Spec Kit to Software Development Processes

**Comprehensive Integration Guidelines for AI-Augmented Development**

**Date**: 2025-10-26
**Version**: 1.0
**Project**: GitHub Spec Kit (SD2-METHOD)

---

## Executive Summary

GitHub Spec Kit is an **AI-augmented framework** that supports software development by providing 8 commands that map to different phases of the Software Development Life Cycle (SDLC). This document provides comprehensive guidelines for integrating Spec Kit into various development methodologies including Agile/Scrum, Kanban, Waterfall, and hybrid approaches.

**Key Value Proposition**: Spec Kit bridges the gap between business requirements and implementation by providing structured, AI-assisted workflows that ensure quality, consistency, and traceability throughout the development process.

---

## Table of Contents

1. [SDLC Phase Mapping](#sdlc-phase-mapping)
2. [Agile/Scrum Integration](#agilescrum-integration)
3. [Kanban Integration](#kanban-integration)
4. [Waterfall Integration](#waterfall-integration)
5. [DevOps/CI-CD Integration](#devopsci-cd-integration)
6. [Team Size Adaptations](#team-size-adaptations)
7. [Project Type Adaptations](#project-type-adaptations)
8. [Quality Gates and Metrics](#quality-gates-and-metrics)
9. [Best Practices and Patterns](#best-practices-and-patterns)
10. [Common Pitfalls and Solutions](#common-pitfalls-and-solutions)
11. [Integration with Existing Tools](#integration-with-existing-tools)
12. [Real-World Workflows](#real-world-workflows)

---

## SDLC Phase Mapping

### Traditional SDLC Phases

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         SDLC PHASE MAPPING                              │
└─────────────────────────────────────────────────────────────────────────┘

Phase 1: REQUIREMENTS GATHERING & ANALYSIS
├─ Activities: Stakeholder interviews, requirements elicitation
├─ Spec Kit Commands:
│  ├─ /speckit.specify (PRIMARY)
│  │  └─ Input: Natural language feature description
│  │  └─ Output: Formal specification (spec.md)
│  │  └─ Duration: 15-30 minutes per feature
│  │
│  └─ /speckit.clarify (PRIMARY)
│     └─ Input: Questions about ambiguous requirements
│     └─ Output: Updated spec.md with clarifications
│     └─ Duration: 10-20 minutes
│
├─ Artifacts Produced: spec.md, requirements.md checklist
├─ Roles: Product Owner, Business Analyst
└─ Exit Criteria: All [NEEDS CLARIFICATION] markers resolved

─────────────────────────────────────────────────────────────────────────

Phase 2: REQUIREMENTS VALIDATION & QUALITY ASSURANCE
├─ Activities: Requirements review, quality checks, stakeholder approval
├─ Spec Kit Commands:
│  └─ /speckit.checklist (PRIMARY - Multi-domain)
│     ├─ UX checklist (Product Owner)
│     ├─ Requirements quality checklist (Business Analyst)
│     ├─ Testing requirements checklist (QA Engineer)
│     ├─ Security requirements checklist (Security Engineer)
│     ├─ Performance requirements checklist (Architect)
│     └─ Compliance checklist (Compliance Officer)
│
├─ Artifacts Produced: Domain-specific checklists (ux.md, security.md, etc.)
├─ Roles: All stakeholders (multi-role collaboration)
└─ Exit Criteria: All checklists 100% complete and approved

─────────────────────────────────────────────────────────────────────────

Phase 3: SYSTEM DESIGN & ARCHITECTURE
├─ Activities: Technical design, architecture decisions, API design
├─ Spec Kit Commands:
│  ├─ /speckit.plan (PRIMARY)
│  │  └─ Input: spec.md
│  │  └─ Output: plan.md, data-model.md, contracts/, research.md
│  │  └─ Duration: 30-60 minutes per feature
│  │
│  └─ /speckit.checklist (SECONDARY - Architecture focus)
│     └─ Output: architecture.md checklist
│
├─ Artifacts Produced: Technical design documents, API contracts, data models
├─ Roles: Solution Architect, Technical Lead
└─ Exit Criteria: Architecture approved, all design decisions documented

─────────────────────────────────────────────────────────────────────────

Phase 4: IMPLEMENTATION PLANNING
├─ Activities: Task breakdown, effort estimation, sprint planning
├─ Spec Kit Commands:
│  └─ /speckit.tasks (PRIMARY)
│     └─ Input: spec.md, plan.md, data-model.md, contracts/
│     └─ Output: tasks.md with dependency graph
│     └─ Duration: 20-40 minutes per feature
│
├─ Artifacts Produced: Executable task list, dependency graph, estimates
├─ Roles: Technical Lead, Scrum Master
└─ Exit Criteria: All tasks defined, dependencies identified, estimates provided

─────────────────────────────────────────────────────────────────────────

Phase 5: PRE-IMPLEMENTATION VALIDATION
├─ Activities: Quality gate, consistency check, risk assessment
├─ Spec Kit Commands:
│  └─ /speckit.analyze (PRIMARY)
│     └─ Input: spec.md, plan.md, tasks.md
│     └─ Output: Analysis report with issues, coverage metrics
│     └─ Duration: 10-15 minutes
│
├─ Artifacts Produced: Analysis report, coverage matrix, issue list
├─ Roles: Technical Lead, QA Engineer
└─ Exit Criteria: 0 critical issues, >90% requirement coverage

─────────────────────────────────────────────────────────────────────────

Phase 6: IMPLEMENTATION & CODING
├─ Activities: Code development, unit testing, code review
├─ Spec Kit Commands:
│  └─ /speckit.implement (PRIMARY)
│     └─ Input: All artifacts (spec, plan, tasks, data-model, contracts)
│     └─ Output: Working code implementation
│     └─ Duration: Varies by feature complexity
│
├─ Artifacts Produced: Source code, tests, documentation
├─ Roles: Software Engineer, Developer
└─ Exit Criteria: All tasks complete, tests pass, code reviewed

─────────────────────────────────────────────────────────────────────────

Phase 0: GOVERNANCE (One-time setup)
├─ Activities: Define project principles, standards, quality gates
├─ Spec Kit Commands:
│  └─ /speckit.constitution (PRIMARY)
│     └─ Input: Project principles and standards
│     └─ Output: constitution.md
│     └─ Duration: 1-2 hours (one-time)
│
├─ Artifacts Produced: Project constitution, governance rules
├─ Roles: Engineering Manager, Solution Architect
└─ Exit Criteria: All principles defined and ratified
```

---

## Agile/Scrum Integration

### Scrum Ceremony Mapping

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    SCRUM CEREMONY INTEGRATION                            │
└─────────────────────────────────────────────────────────────────────────┘

SPRINT 0: Project Initialization
├─ Activity: Define project constitution and standards
├─ Spec Kit Usage:
│  └─ Engineering Manager: /speckit.constitution
│     └─ Define: Testing standards, architecture principles, quality gates
│     └─ Output: constitution.md
│
└─ Duration: 1-2 hours (one-time per project)

═══════════════════════════════════════════════════════════════════════════

BACKLOG REFINEMENT (Weekly)
├─ Activity: Elaborate user stories, estimate effort
├─ Spec Kit Usage:
│  ├─ Product Owner: /speckit.specify
│  │  └─ For each user story in backlog
│  │  └─ Creates: spec.md per story
│  │
│  ├─ Product Owner: /speckit.clarify
│  │  └─ Answers questions about scope, acceptance criteria
│  │  └─ Updates: spec.md with clarifications
│  │
│  ├─ Team: /speckit.checklist (multi-domain)
│  │  └─ QA: Testing requirements
│  │  └─ Security: Security requirements
│  │  └─ DevOps: Deployment requirements
│  │
│  ├─ Solution Architect: /speckit.plan
│  │  └─ Creates technical design
│  │  └─ Output: plan.md, data-model.md, contracts/
│  │
│  └─ Tech Lead: /speckit.tasks
│     └─ Breaks down into tasks with estimates
│     └─ Output: tasks.md with story points
│
├─ Output: "Ready" stories with all artifacts complete
└─ Duration: 2-3 hours per week

BEST PRACTICE: Refine stories 1-2 sprints ahead of implementation

═══════════════════════════════════════════════════════════════════════════

SPRINT PLANNING (Start of sprint)
├─ Activity: Select stories, commit to sprint goal
├─ Spec Kit Usage:
│  ├─ Scrum Master: Reviews tasks.md for each story
│  │  └─ Identifies: Parallel work opportunities
│  │  └─ Identifies: Dependencies and blockers
│  │  └─ Estimates: Team capacity vs. story points
│  │
│  └─ Tech Lead: /speckit.analyze
│     └─ Validates: All stories ready for implementation
│     └─ Output: Pre-sprint quality report
│
├─ Decision: Which stories to include in sprint
└─ Duration: 2-4 hours

SPRINT PLANNING PART 1 (What to build):
  - Review spec.md for each story
  - Confirm acceptance criteria with Product Owner
  - Verify all checklists complete

SPRINT PLANNING PART 2 (How to build):
  - Review plan.md for technical approach
  - Review tasks.md for task breakdown
  - Assign tasks to developers based on parallel opportunities

═══════════════════════════════════════════════════════════════════════════

DAILY STANDUP (Every day)
├─ Activity: Synchronize team, identify blockers
├─ Spec Kit Usage:
│  └─ Team: Reviews tasks.md
│     ├─ "Yesterday: Completed tasks T001-T005"
│     ├─ "Today: Working on tasks T006-T008"
│     └─ "Blockers: T009 needs clarification"
│
├─ Blocker Resolution:
│  └─ If requirements unclear: /speckit.clarify
│     └─ Developer or Product Owner runs immediately
│     └─ Updates spec.md
│     └─ Re-generates tasks if needed: /speckit.tasks
│
└─ Duration: 15 minutes

BEST PRACTICE: Mark completed tasks in tasks.md before standup

═══════════════════════════════════════════════════════════════════════════

SPRINT IMPLEMENTATION (Throughout sprint)
├─ Activity: Code development
├─ Spec Kit Usage:
│  ├─ Developer: /speckit.implement
│  │  └─ Checks: All checklists complete (gates)
│  │  └─ Executes: Tasks phase by phase
│  │  └─ Marks: Tasks complete in tasks.md
│  │
│  └─ Developer: /speckit.clarify (as needed)
│     └─ When: Ambiguity discovered during coding
│     └─ Result: Spec updated, tasks regenerated if needed
│
└─ Duration: Sprint length (1-2 weeks)

RECOMMENDED WORKFLOW:
  Day 1-2: Setup and foundational tasks
  Day 3-8: User story implementation (prioritized order)
  Day 9-10: Polish, testing, code review

═══════════════════════════════════════════════════════════════════════════

SPRINT REVIEW (End of sprint)
├─ Activity: Demo completed work to stakeholders
├─ Spec Kit Usage:
│  └─ Product Owner: Validates against spec.md
│     ├─ Checks: All functional requirements met
│     ├─ Checks: Acceptance criteria satisfied
│     └─ Checks: Success criteria achieved
│
├─ Acceptance:
│  ├─ PASS: Story accepted, mark complete in backlog
│  └─ FAIL: Identify gaps, create follow-up stories
│
└─ Duration: 1-2 hours

═══════════════════════════════════════════════════════════════════════════

SPRINT RETROSPECTIVE (End of sprint)
├─ Activity: Process improvement
├─ Spec Kit Usage:
│  └─ Team Discussion Points:
│     ├─ "Were specs clear enough?" → Improve /speckit.specify usage
│     ├─ "Did we catch issues early?" → Improve /speckit.analyze usage
│     ├─ "Were tasks well-sized?" → Improve /speckit.tasks usage
│     └─ "Do we need new principles?" → Update /speckit.constitution
│
├─ Action Items:
│  └─ Update constitution.md with new standards if needed
│
└─ Duration: 1-2 hours
```

---

### Sprint-Level Workflow Example

**2-Week Sprint with 3 User Stories**

```
WEEK BEFORE SPRINT (Backlog Refinement):
┌────────────────────────────────────────────────────────────────┐
│ Monday-Wednesday: Story Elaboration                             │
├────────────────────────────────────────────────────────────────┤
│ Story 1: User Registration (P1)                                │
│   PO: /speckit.specify → /speckit.clarify                      │
│   Team: /speckit.checklist (UX, Security, Testing)             │
│   Architect: /speckit.plan                                     │
│   Tech Lead: /speckit.tasks → /speckit.analyze                 │
│   Status: ✓ Ready (8 tasks, 13 story points)                  │
├────────────────────────────────────────────────────────────────┤
│ Story 2: User Login (P1)                                       │
│   [Same workflow]                                              │
│   Status: ✓ Ready (6 tasks, 8 story points)                   │
├────────────────────────────────────────────────────────────────┤
│ Story 3: Password Reset (P2)                                   │
│   [Same workflow]                                              │
│   Status: ✓ Ready (7 tasks, 8 story points)                   │
└────────────────────────────────────────────────────────────────┘
Total: 21 tasks, 29 story points

SPRINT PLANNING (Friday):
┌────────────────────────────────────────────────────────────────┐
│ Part 1: What (1 hour)                                          │
│   - Review spec.md for all 3 stories                           │
│   - Confirm acceptance criteria                                │
│   - Sprint Goal: "Complete user authentication foundation"     │
│   - Commitment: Story 1 + Story 2 (team velocity: 20 points)   │
├────────────────────────────────────────────────────────────────┤
│ Part 2: How (1 hour)                                           │
│   - Review plan.md, tasks.md                                   │
│   - Tech Lead runs /speckit.analyze for Story 1 & 2            │
│   - Assign tasks:                                              │
│     • Dev A: Frontend tasks (5 parallel tasks)                 │
│     • Dev B: Backend API (4 parallel tasks)                    │
│     • Dev C: Database & auth (5 sequential tasks)              │
└────────────────────────────────────────────────────────────────┘

SPRINT EXECUTION (Week 1):
┌────────────────────────────────────────────────────────────────┐
│ Monday (Day 1):                                                │
│   Dev A, B, C: /speckit.implement (Story 1)                    │
│   - Setup phase: 3 tasks completed                             │
│   Daily Standup: No blockers                                   │
├────────────────────────────────────────────────────────────────┤
│ Tuesday (Day 2):                                               │
│   Continue implementation                                      │
│   - 4 more tasks completed (7/8 done)                          │
│   Daily Standup: Dev B needs clarification on email validation │
│   - PO: /speckit.clarify (resolved in 5 minutes)               │
├────────────────────────────────────────────────────────────────┤
│ Wednesday (Day 3):                                             │
│   Story 1 complete (8/8 tasks)                                 │
│   Start Story 2                                                │
│   Daily Standup: No blockers                                   │
├────────────────────────────────────────────────────────────────┤
│ Thursday (Day 4):                                              │
│   Story 2 implementation (4/6 tasks)                           │
│   Daily Standup: Dev C waiting on Story 1 review               │
├────────────────────────────────────────────────────────────────┤
│ Friday (Day 5):                                                │
│   Story 2 complete (6/6 tasks)                                 │
│   Code review, testing                                         │
│   Daily Standup: Ready for sprint review                       │
└────────────────────────────────────────────────────────────────┘

SPRINT EXECUTION (Week 2):
┌────────────────────────────────────────────────────────────────┐
│ Monday (Day 6): Code review, bug fixes                         │
│ Tuesday (Day 7): Integration testing                           │
│ Wednesday (Day 8): Demo preparation                            │
│ Thursday (Day 9): Buffer for unexpected issues                 │
│ Friday (Day 10): Sprint Review + Retrospective                 │
│   - Review: Both stories accepted ✓                            │
│   - Retrospective: Team velocity was accurate                  │
└────────────────────────────────────────────────────────────────┘
```

---

## Kanban Integration

### Kanban Board with Spec Kit

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         KANBAN BOARD COLUMNS                             │
└─────────────────────────────────────────────────────────────────────────┘

Column 1: BACKLOG
├─ WIP Limit: Unlimited
├─ Entry Criteria: Raw user story or feature request
├─ Spec Kit Actions: None yet
└─ Exit Criteria: Ready for specification

─────────────────────────────────────────────────────────────────────────

Column 2: SPECIFICATION IN PROGRESS
├─ WIP Limit: 3 stories
├─ Entry Criteria: Story selected for elaboration
├─ Spec Kit Actions:
│  ├─ Product Owner: /speckit.specify
│  └─ Product Owner: /speckit.clarify
│
├─ Definition of Done for column:
│  ├─ spec.md created
│  ├─ All [NEEDS CLARIFICATION] markers resolved
│  └─ requirements.md checklist created
│
└─ Exit Criteria: Specification complete, ready for validation

─────────────────────────────────────────────────────────────────────────

Column 3: SPECIFICATION VALIDATION
├─ WIP Limit: 5 stories
├─ Entry Criteria: Specification complete
├─ Spec Kit Actions:
│  └─ Multi-role: /speckit.checklist (parallel by domain)
│     ├─ QA Engineer: Testing checklist
│     ├─ Security Engineer: Security checklist
│     ├─ DevOps: Deployment checklist
│     └─ Architect: Architecture checklist
│
├─ Definition of Done for column:
│  └─ All checklists 100% complete
│
└─ Exit Criteria: Requirements validated, ready for design

─────────────────────────────────────────────────────────────────────────

Column 4: TECHNICAL DESIGN
├─ WIP Limit: 3 stories
├─ Entry Criteria: All checklists complete
├─ Spec Kit Actions:
│  ├─ Solution Architect: /speckit.plan
│  └─ Solution Architect: /speckit.checklist (architecture focus)
│
├─ Definition of Done for column:
│  ├─ plan.md created
│  ├─ data-model.md created
│  ├─ contracts/ created
│  ├─ research.md created
│  └─ Architecture checklist complete
│
└─ Exit Criteria: Technical design approved, ready for task breakdown

─────────────────────────────────────────────────────────────────────────

Column 5: TASK PLANNING
├─ WIP Limit: 4 stories
├─ Entry Criteria: Technical design complete
├─ Spec Kit Actions:
│  ├─ Tech Lead: /speckit.tasks
│  └─ Tech Lead: /speckit.analyze
│
├─ Definition of Done for column:
│  ├─ tasks.md created with all tasks
│  ├─ Dependency graph generated
│  ├─ /speckit.analyze shows 0 critical issues
│  └─ Effort estimated
│
└─ Exit Criteria: Ready for implementation, analysis passed

─────────────────────────────────────────────────────────────────────────

Column 6: READY FOR DEVELOPMENT
├─ WIP Limit: 6 stories (depends on team size)
├─ Entry Criteria: All artifacts complete, analysis passed
├─ Spec Kit Actions: None (waiting for developer availability)
├─ Definition of Done for column: N/A (queue)
└─ Exit Criteria: Developer pulls story into In Progress

─────────────────────────────────────────────────────────────────────────

Column 7: IN DEVELOPMENT
├─ WIP Limit: 1 per developer (e.g., 3 for 3 developers)
├─ Entry Criteria: Developer available, story pulled from Ready
├─ Spec Kit Actions:
│  ├─ Developer: /speckit.implement
│  └─ Developer: /speckit.clarify (if needed)
│
├─ Definition of Done for column:
│  ├─ All tasks in tasks.md marked complete [X]
│  ├─ Code written and committed
│  ├─ Unit tests pass
│  └─ Self-review complete
│
└─ Exit Criteria: Code ready for review

─────────────────────────────────────────────────────────────────────────

Column 8: CODE REVIEW
├─ WIP Limit: 4 stories
├─ Entry Criteria: Development complete
├─ Spec Kit Actions:
│  └─ Reviewer validates against:
│     ├─ spec.md (requirements)
│     ├─ plan.md (architecture)
│     ├─ tasks.md (completeness)
│     └─ All checklists
│
├─ Definition of Done for column:
│  ├─ Code reviewed and approved
│  ├─ All comments addressed
│  └─ Meets coding standards
│
└─ Exit Criteria: Approved, ready for testing

─────────────────────────────────────────────────────────────────────────

Column 9: TESTING
├─ WIP Limit: 5 stories
├─ Entry Criteria: Code review approved
├─ Spec Kit Actions:
│  └─ QA validates against:
│     ├─ spec.md (functional requirements)
│     ├─ testing.md checklist
│     └─ Acceptance criteria
│
├─ Definition of Done for column:
│  ├─ All tests pass
│  ├─ Acceptance criteria met
│  └─ No critical bugs
│
└─ Exit Criteria: Testing passed, ready for deployment

─────────────────────────────────────────────────────────────────────────

Column 10: DONE
├─ WIP Limit: Unlimited
├─ Entry Criteria: Testing passed, deployed to production
├─ Spec Kit Actions: None
└─ Exit Criteria: N/A (final state)
```

---

### Kanban Metrics with Spec Kit

```
CYCLE TIME BREAKDOWN BY COLUMN (Target vs. Actual):

┌─────────────────────────────────────────┬──────────┬──────────┐
│ Column                                  │ Target   │ Actual   │
├─────────────────────────────────────────┼──────────┼──────────┤
│ Specification In Progress               │ 4 hours  │ 3 hours  │
│ (/speckit.specify + /speckit.clarify)   │          │          │
├─────────────────────────────────────────┼──────────┼──────────┤
│ Specification Validation                │ 8 hours  │ 6 hours  │
│ (/speckit.checklist - multi-domain)     │          │          │
├─────────────────────────────────────────┼──────────┼──────────┤
│ Technical Design                        │ 6 hours  │ 4 hours  │
│ (/speckit.plan)                         │          │          │
├─────────────────────────────────────────┼──────────┼──────────┤
│ Task Planning                           │ 3 hours  │ 2 hours  │
│ (/speckit.tasks + /speckit.analyze)     │          │          │
├─────────────────────────────────────────┼──────────┼──────────┤
│ Ready for Development (queue time)      │ 24 hours │ 12 hours │
├─────────────────────────────────────────┼──────────┼──────────┤
│ In Development                          │ 3 days   │ 4 days   │
│ (/speckit.implement)                    │          │          │
├─────────────────────────────────────────┼──────────┼──────────┤
│ Code Review                             │ 4 hours  │ 8 hours  │
├─────────────────────────────────────────┼──────────┼──────────┤
│ Testing                                 │ 1 day    │ 1 day    │
└─────────────────────────────────────────┴──────────┴──────────┘

TOTAL CYCLE TIME: Target 5.5 days | Actual 6 days

BOTTLENECK ANALYSIS:
- Code Review taking 2x longer → Consider pair programming
- Development slower than expected → Re-estimate task complexity
```

---

## Waterfall Integration

### Waterfall Phases with Spec Kit

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    WATERFALL METHODOLOGY INTEGRATION                     │
└─────────────────────────────────────────────────────────────────────────┘

PHASE 1: REQUIREMENTS GATHERING & ANALYSIS (Weeks 1-4)
├─ Traditional Activities:
│  ├─ Stakeholder interviews
│  ├─ Business requirements documentation
│  ├─ System requirements specification
│  └─ Requirements review meetings
│
├─ Spec Kit Integration:
│  ├─ Week 1-2: Feature specification
│  │  ├─ Business Analyst: /speckit.specify (for each feature)
│  │  └─ Product Owner: /speckit.clarify (resolve all ambiguities)
│  │
│  └─ Week 3-4: Requirements validation
│     └─ All stakeholders: /speckit.checklist (comprehensive)
│        ├─ Business requirements checklist
│        ├─ Functional requirements checklist
│        ├─ Non-functional requirements checklist
│        ├─ Security requirements checklist
│        ├─ Compliance requirements checklist
│        └─ User experience requirements checklist
│
├─ Deliverables:
│  ├─ Complete spec.md for all features
│  ├─ All checklists 100% complete
│  └─ Requirements Specification Document (RSD) approved
│
├─ Quality Gate:
│  └─ GATE 1: Requirements Sign-Off
│     ├─ All stakeholders approve spec.md
│     ├─ All checklists completed
│     ├─ No [NEEDS CLARIFICATION] markers remain
│     └─ Business case validated
│
└─ Duration: 4 weeks

─────────────────────────────────────────────────────────────────────────

PHASE 2: SYSTEM DESIGN (Weeks 5-8)
├─ Traditional Activities:
│  ├─ High-level design (HLD)
│  ├─ Low-level design (LLD)
│  ├─ Database design
│  ├─ API design
│  ├─ Architecture review
│  └─ Design review meetings
│
├─ Spec Kit Integration:
│  ├─ Week 5: Project governance
│  │  └─ Engineering Manager: /speckit.constitution
│  │     └─ Define all project principles and standards
│  │
│  ├─ Week 5-7: Technical design
│  │  └─ Solution Architect: /speckit.plan (for all features)
│  │     └─ Generates:
│  │        ├─ research.md (technology decisions)
│  │        ├─ plan.md (implementation plan)
│  │        ├─ data-model.md (complete database schema)
│  │        └─ contracts/ (all API specifications)
│  │
│  └─ Week 8: Design validation
│     └─ Solution Architect: /speckit.checklist (architecture)
│        └─ Validates scalability, security, maintainability
│
├─ Deliverables:
│  ├─ constitution.md (project governance)
│  ├─ Complete plan.md for all features
│  ├─ Complete data-model.md
│  ├─ All API contracts in contracts/
│  ├─ Architecture checklist complete
│  └─ System Design Document (SDD) approved
│
├─ Quality Gate:
│  └─ GATE 2: Design Sign-Off
│     ├─ Architecture review board approval
│     ├─ All design decisions documented
│     ├─ Constitution principles defined
│     ├─ Security review passed
│     └─ Performance estimates validated
│
└─ Duration: 4 weeks

─────────────────────────────────────────────────────────────────────────

PHASE 3: IMPLEMENTATION PLANNING (Week 9)
├─ Traditional Activities:
│  ├─ Work breakdown structure (WBS)
│  ├─ Resource allocation
│  ├─ Schedule development
│  └─ Risk assessment
│
├─ Spec Kit Integration:
│  └─ Tech Lead: /speckit.tasks (for all features)
│     └─ Generates complete tasks.md with:
│        ├─ All implementation tasks
│        ├─ Dependency graph
│        ├─ Parallel execution opportunities
│        └─ Effort estimates
│
├─ Deliverables:
│  ├─ Complete tasks.md for entire project
│  ├─ Project schedule (Gantt chart)
│  ├─ Resource allocation plan
│  └─ Risk mitigation plan
│
├─ Quality Gate:
│  └─ GATE 3: Implementation Planning Sign-Off
│     ├─ All tasks defined and estimated
│     ├─ Dependencies identified
│     ├─ Schedule approved by stakeholders
│     └─ Resources committed
│
└─ Duration: 1 week

─────────────────────────────────────────────────────────────────────────

PHASE 4: PRE-IMPLEMENTATION VALIDATION (Week 10)
├─ Traditional Activities:
│  ├─ Final review of all artifacts
│  ├─ Risk assessment
│  └─ Go/No-Go decision
│
├─ Spec Kit Integration:
│  └─ Tech Lead: /speckit.analyze (entire project)
│     └─ Validates:
│        ├─ Spec-plan-tasks consistency
│        ├─ Constitution compliance
│        ├─ Requirement coverage (100%)
│        ├─ No ambiguities
│        └─ No critical issues
│
├─ Deliverables:
│  ├─ Analysis report
│  ├─ Issue resolution log
│  └─ Go/No-Go decision document
│
├─ Quality Gate:
│  └─ GATE 4: Implementation Readiness
│     ├─ /speckit.analyze shows 0 critical issues
│     ├─ All artifacts complete
│     ├─ Development environment ready
│     └─ Team trained and ready
│
└─ Duration: 1 week

─────────────────────────────────────────────────────────────────────────

PHASE 5: IMPLEMENTATION (Weeks 11-24)
├─ Traditional Activities:
│  ├─ Coding
│  ├─ Unit testing
│  ├─ Code reviews
│  └─ Integration
│
├─ Spec Kit Integration:
│  └─ Developers: /speckit.implement (by feature/module)
│     ├─ Week 11-12: Setup and foundational modules
│     ├─ Week 13-20: Core feature implementation
│     ├─ Week 21-22: Integration and polish
│     └─ Week 23-24: Bug fixes and final integration
│
├─ Deliverables:
│  ├─ Source code
│  ├─ Unit test results
│  ├─ Code review records
│  └─ Build artifacts
│
├─ Quality Gate:
│  └─ GATE 5: Implementation Complete
│     ├─ All tasks.md tasks marked complete [X]
│     ├─ All unit tests pass
│     ├─ Code review completed
│     ├─ Code coverage meets standards
│     └─ No critical bugs
│
└─ Duration: 14 weeks

─────────────────────────────────────────────────────────────────────────

PHASE 6: TESTING (Weeks 25-28)
├─ Traditional Activities:
│  ├─ Integration testing
│  ├─ System testing
│  ├─ User acceptance testing (UAT)
│  └─ Performance testing
│
├─ Spec Kit Integration:
│  └─ QA Team validates against:
│     ├─ spec.md (all functional requirements)
│     ├─ All checklists (testing, security, performance)
│     ├─ Acceptance criteria in spec.md
│     └─ Success criteria in spec.md
│
├─ Deliverables:
│  ├─ Test plans and test cases
│  ├─ Test execution reports
│  ├─ Defect logs
│  └─ UAT sign-off
│
├─ Quality Gate:
│  └─ GATE 6: Testing Complete
│     ├─ All test cases pass
│     ├─ All critical bugs resolved
│     ├─ Performance meets requirements
│     ├─ Security scan clean
│     └─ UAT approved
│
└─ Duration: 4 weeks

─────────────────────────────────────────────────────────────────────────

PHASE 7: DEPLOYMENT (Week 29)
├─ Traditional Activities:
│  ├─ Production deployment
│  ├─ Data migration
│  ├─ Training
│  └─ Handover to operations
│
├─ Spec Kit Integration:
│  └─ DevOps validates deployment checklist
│     └─ Deployment readiness checklist from refinement phase
│
├─ Deliverables:
│  ├─ Production deployment
│  ├─ User documentation
│  ├─ Operations manual
│  └─ Training materials
│
├─ Quality Gate:
│  └─ GATE 7: Production Readiness
│     ├─ Deployment successful
│     ├─ Smoke tests pass
│     ├─ Monitoring in place
│     └─ Support team trained
│
└─ Duration: 1 week

─────────────────────────────────────────────────────────────────────────

PHASE 8: MAINTENANCE (Ongoing)
├─ Traditional Activities:
│  ├─ Bug fixes
│  ├─ Minor enhancements
│  ├─ Performance tuning
│  └─ User support
│
├─ Spec Kit Integration:
│  └─ For each change:
│     ├─ /speckit.specify (document change)
│     ├─ /speckit.clarify (if needed)
│     ├─ /speckit.plan (if architectural impact)
│     ├─ /speckit.tasks (break down work)
│     ├─ /speckit.analyze (validate consistency)
│     └─ /speckit.implement (execute change)
│
└─ Duration: Ongoing post-launch
```

---

## DevOps/CI-CD Integration

### Continuous Integration Pipeline

```
┌─────────────────────────────────────────────────────────────────────────┐
│                      CI/CD PIPELINE INTEGRATION                          │
└─────────────────────────────────────────────────────────────────────────┘

STAGE 1: SPECIFICATION VALIDATION (Pre-Pipeline)
├─ Trigger: Pull request to add/modify spec.md
├─ Automated Checks:
│  ├─ Check 1: All [NEEDS CLARIFICATION] markers resolved
│  ├─ Check 2: All mandatory sections present
│  ├─ Check 3: All checklists exist in checklists/ directory
│  ├─ Check 4: All checklists 100% complete (no unchecked items)
│  └─ Check 5: spec.md passes markdown linting
│
├─ Quality Gate:
│  └─ GATE: Specification Valid
│     └─ Status: ✓ PASS / ✗ FAIL (blocks merge)
│
└─ Action: Auto-comment on PR with validation results

─────────────────────────────────────────────────────────────────────────

STAGE 2: DESIGN VALIDATION (Pre-Pipeline)
├─ Trigger: Pull request to add/modify plan.md or data-model.md
├─ Automated Checks:
│  ├─ Check 1: plan.md references valid spec.md
│  ├─ Check 2: data-model.md syntax valid
│  ├─ Check 3: contracts/ files are valid OpenAPI/GraphQL schemas
│  ├─ Check 4: All design artifacts present (plan, data-model, contracts)
│  └─ Check 5: Architecture checklist complete
│
├─ Quality Gate:
│  └─ GATE: Design Valid
│     └─ Status: ✓ PASS / ✗ FAIL (blocks merge)
│
└─ Action: Auto-comment on PR with validation results

─────────────────────────────────────────────────────────────────────────

STAGE 3: TASK VALIDATION (Pre-Pipeline)
├─ Trigger: Pull request to add/modify tasks.md
├─ Automated Checks:
│  ├─ Check 1: tasks.md format valid (all tasks have checkbox, ID, description)
│  ├─ Check 2: All task IDs are sequential (T001, T002, etc.)
│  ├─ Check 3: All referenced files exist or are planned
│  └─ Check 4: Story labels [US1], [US2] match spec.md user stories
│
├─ Quality Gate:
│  └─ GATE: Tasks Valid
│     └─ Status: ✓ PASS / ✗ FAIL (blocks merge)
│
└─ Action: Auto-comment on PR with validation results

─────────────────────────────────────────────────────────────────────────

STAGE 4: ANALYSIS (Pre-Implementation Gate)
├─ Trigger: Manual or scheduled (before sprint starts)
├─ Action: Run /speckit.analyze via CLI or API
├─ Automated Checks:
│  ├─ Spec-plan-tasks consistency
│  ├─ Constitution compliance
│  ├─ Requirement coverage percentage
│  ├─ Task coverage percentage
│  ├─ Critical issue count
│  └─ Ambiguity count
│
├─ Quality Gate:
│  └─ GATE: Analysis Passed
│     ├─ Critical issues: 0
│     ├─ Requirement coverage: ≥90%
│     ├─ Task coverage: ≥90%
│     └─ Status: ✓ PASS / ✗ FAIL (blocks sprint start)
│
└─ Action: Generate analysis report, post to project management tool

─────────────────────────────────────────────────────────────────────────

STAGE 5: BUILD (Standard CI)
├─ Trigger: Push to feature branch
├─ Actions:
│  ├─ Checkout code
│  ├─ Install dependencies
│  ├─ Compile/Build
│  └─ Run linters
│
└─ Quality Gate: Build succeeds

─────────────────────────────────────────────────────────────────────────

STAGE 6: UNIT TESTS (Standard CI)
├─ Trigger: After build succeeds
├─ Actions:
│  ├─ Run unit tests
│  ├─ Generate coverage report
│  └─ Check coverage threshold
│
├─ Spec Kit Integration:
│  └─ Validate tests against:
│     └─ contracts/ (API contract tests)
│     └─ data-model.md (entity tests)
│
└─ Quality Gate:
│  ├─ All tests pass
│  └─ Coverage ≥ constitution.md threshold (e.g., 80%)

─────────────────────────────────────────────────────────────────────────

STAGE 7: INTEGRATION TESTS
├─ Trigger: After unit tests pass
├─ Actions:
│  ├─ Deploy to test environment
│  ├─ Run integration tests
│  └─ Run contract tests against contracts/ specs
│
├─ Spec Kit Integration:
│  └─ Validate API responses against contracts/ schemas
│
└─ Quality Gate: All integration tests pass

─────────────────────────────────────────────────────────────────────────

STAGE 8: SECURITY SCAN
├─ Trigger: After integration tests pass
├─ Actions:
│  ├─ SAST (Static Application Security Testing)
│  ├─ Dependency vulnerability scan
│  ├─ Secret scanning
│  └─ License compliance check
│
├─ Spec Kit Integration:
│  └─ Validate against security.md checklist requirements
│
└─ Quality Gate:
│  ├─ No critical vulnerabilities
│  ├─ No high-severity vulnerabilities (or approved exceptions)
│  └─ No secrets in code

─────────────────────────────────────────────────────────────────────────

STAGE 9: PERFORMANCE TESTS
├─ Trigger: After security scan passes (for critical paths only)
├─ Actions:
│  ├─ Load testing
│  ├─ Stress testing
│  └─ Latency testing
│
├─ Spec Kit Integration:
│  └─ Validate against performance requirements in spec.md
│     └─ Example: "95% of requests complete in <200ms"
│
└─ Quality Gate: Performance meets spec.md requirements

─────────────────────────────────────────────────────────────────────────

STAGE 10: ACCEPTANCE VALIDATION
├─ Trigger: Manual (before merge to main)
├─ Actions:
│  └─ Validate implementation against spec.md acceptance criteria
│
├─ Spec Kit Integration:
│  └─ Automated script checks:
│     ├─ All functional requirements implemented
│     ├─ All user stories have passing tests
│     ├─ All success criteria met
│     └─ All checklists validated
│
└─ Quality Gate:
│  └─ GATE: Acceptance Criteria Met
│     └─ Status: ✓ PASS / ✗ FAIL (blocks merge)

─────────────────────────────────────────────────────────────────────────

STAGE 11: DEPLOYMENT (CD)
├─ Trigger: Merge to main branch
├─ Actions:
│  ├─ Deploy to staging
│  ├─ Smoke tests
│  ├─ Deploy to production (with approval)
│  └─ Monitor metrics
│
└─ Quality Gate: Deployment successful, smoke tests pass
```

---

### CI/CD Configuration Example

**.github/workflows/spec-kit-validation.yml**

```yaml
name: Spec Kit Validation

on:
  pull_request:
    paths:
      - 'specs/**'
      - 'memory/constitution.md'

jobs:
  validate-specification:
    runs-on: ubuntu-latest
    if: contains(github.event.pull_request.changed_files, 'spec.md')
    steps:
      - uses: actions/checkout@v3

      - name: Check for NEEDS CLARIFICATION markers
        run: |
          if grep -r "NEEDS CLARIFICATION" specs/*/spec.md; then
            echo "❌ Found unresolved clarification markers"
            exit 1
          fi
          echo "✓ No clarification markers found"

      - name: Validate mandatory sections
        run: |
          for spec in specs/*/spec.md; do
            required_sections=("Overview" "Functional Requirements" "User Stories" "Success Criteria")
            for section in "${required_sections[@]}"; do
              if ! grep -q "## $section" "$spec"; then
                echo "❌ Missing section: $section in $spec"
                exit 1
              fi
            done
          done
          echo "✓ All mandatory sections present"

      - name: Check checklists exist and complete
        run: |
          for feature_dir in specs/*/; do
            if [ -f "${feature_dir}spec.md" ]; then
              if [ ! -d "${feature_dir}checklists" ]; then
                echo "❌ No checklists directory in $feature_dir"
                exit 1
              fi

              # Check for incomplete checklist items
              if grep -r "\- \[ \]" "${feature_dir}checklists/" 2>/dev/null; then
                echo "❌ Found incomplete checklist items in $feature_dir"
                exit 1
              fi
            fi
          done
          echo "✓ All checklists complete"

  validate-design:
    runs-on: ubuntu-latest
    if: contains(github.event.pull_request.changed_files, 'plan.md')
    steps:
      - uses: actions/checkout@v3

      - name: Validate plan references spec
        run: |
          for plan in specs/*/plan.md; do
            dir=$(dirname "$plan")
            if [ ! -f "$dir/spec.md" ]; then
              echo "❌ plan.md exists but spec.md missing in $dir"
              exit 1
            fi
          done
          echo "✓ All plans have corresponding specs"

      - name: Validate OpenAPI contracts
        uses: openapi-lint/action@v1
        with:
          paths: 'specs/*/contracts/*.yaml'

  validate-tasks:
    runs-on: ubuntu-latest
    if: contains(github.event.pull_request.changed_files, 'tasks.md')
    steps:
      - uses: actions/checkout@v3

      - name: Validate task format
        run: |
          for tasks in specs/*/tasks.md; do
            # Check all tasks have checkboxes and IDs
            if grep -E "^- \[[ X]\]" "$tasks" | grep -v "T[0-9]{3}" ; then
              echo "❌ Invalid task format in $tasks"
              exit 1
            fi
          done
          echo "✓ All tasks properly formatted"

      - name: Validate story labels match spec
        run: |
          for dir in specs/*/; do
            if [ -f "$dir/tasks.md" ] && [ -f "$dir/spec.md" ]; then
              # Extract user story labels from tasks
              task_stories=$(grep -oE "\[US[0-9]+\]" "$dir/tasks.md" | sort -u)

              # Extract user stories from spec
              spec_stories=$(grep -E "^### User Story [0-9]+" "$dir/spec.md" | wc -l)

              # Validate count matches
              task_count=$(echo "$task_stories" | wc -l)
              if [ $task_count -ne $spec_stories ]; then
                echo "❌ Story count mismatch in $dir"
                echo "Spec has $spec_stories stories, tasks reference $task_count"
                exit 1
              fi
            fi
          done
          echo "✓ Story labels match specifications"

  run-analysis:
    runs-on: ubuntu-latest
    if: github.event.pull_request.label == 'ready-for-implementation'
    steps:
      - uses: actions/checkout@v3

      - name: Setup Spec Kit CLI
        run: |
          pip install specify-cli

      - name: Run spec kit analysis
        run: |
          cd specs/*
          specify analyze > analysis-report.md

          # Check for critical issues
          critical_count=$(grep -c "CRITICAL" analysis-report.md || true)
          if [ $critical_count -gt 0 ]; then
            echo "❌ Found $critical_count critical issues"
            cat analysis-report.md
            exit 1
          fi

          echo "✓ Analysis passed with no critical issues"

      - name: Post analysis report
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('analysis-report.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## Spec Kit Analysis Report\n\n${report}`
            });
```

---

## Team Size Adaptations

### Solo Developer / Small Team (1-3 people)

```
┌─────────────────────────────────────────────────────────────────────────┐
│                     SOLO DEVELOPER / SMALL TEAM                          │
└─────────────────────────────────────────────────────────────────────────┘

Team Size: 1-3 developers
Project Complexity: Small to medium features
Timeline: Days to weeks

SIMPLIFIED WORKFLOW:

Developer (wearing all hats):
├─ 1. Specification (15 minutes)
│  ├─ /speckit.specify
│  └─ /speckit.clarify (self-answer or skip minor clarifications)
│
├─ 2. Quick Validation (10 minutes)
│  └─ /speckit.checklist (focus on critical domains only)
│     ├─ Security (if applicable)
│     └─ Testing (if TDD)
│
├─ 3. Technical Design (20 minutes)
│  └─ /speckit.plan
│
├─ 4. Task Breakdown (10 minutes)
│  └─ /speckit.tasks
│
├─ 5. Quick Analysis (5 minutes)
│  └─ /speckit.analyze
│
└─ 6. Implementation (varies)
   └─ /speckit.implement

TOTAL OVERHEAD: ~1 hour of specification work
BENEFIT: Clear direction, testable requirements, no scope creep

OPTIMIZATIONS:
- Skip /speckit.clarify if requirements are clear
- Generate only essential checklists (1-2 max)
- Combine design and task planning in one session
- Run /speckit.analyze only for complex features
- Focus on speed while maintaining quality

WHEN TO USE FULL WORKFLOW:
- Customer-facing features
- Complex business logic
- Security-sensitive features
- Features with unclear requirements
```

---

### Medium Team (4-10 people)

```
┌─────────────────────────────────────────────────────────────────────────┐
│                          MEDIUM TEAM (4-10)                              │
└─────────────────────────────────────────────────────────────────────────┘

Team Composition:
├─ 1 Product Owner
├─ 1 Tech Lead / Architect
├─ 4-8 Developers
└─ 1 QA Engineer (part-time or full-time)

ROLE DISTRIBUTION:

Product Owner (20% time on Spec Kit):
├─ /speckit.specify (for all new features)
├─ /speckit.clarify (answer questions)
└─ /speckit.checklist (UX focus)

Tech Lead (30% time on Spec Kit):
├─ /speckit.plan (review or create)
├─ /speckit.tasks (for all features)
├─ /speckit.analyze (before each sprint)
└─ /speckit.checklist (architecture focus)

Developers (10% time on Spec Kit):
├─ /speckit.implement (primary activity)
└─ /speckit.clarify (when encountering ambiguity)

QA Engineer (15% time on Spec Kit):
├─ /speckit.checklist (testing focus)
└─ /speckit.analyze (quality validation)

WORKFLOW:

Sprint Planning Preparation (1 week before):
├─ Monday-Tuesday: Product Owner creates specs
│  └─ /speckit.specify + /speckit.clarify
│
├─ Wednesday: Multi-role checklist generation
│  ├─ Product Owner: UX checklist
│  ├─ QA: Testing checklist
│  └─ Tech Lead: Architecture checklist (if needed)
│
├─ Thursday: Tech Lead creates designs
│  └─ /speckit.plan
│
└─ Friday: Tech Lead creates task breakdown
   ├─ /speckit.tasks
   └─ /speckit.analyze

Sprint Execution:
├─ Day 1: Sprint planning (select stories from ready backlog)
├─ Day 2-9: Developers implement
│  └─ /speckit.implement
└─ Day 10: Sprint review and retrospective

PARALLEL WORK OPPORTUNITIES:
- Multiple developers can work on different stories simultaneously
- Tech Lead prepares future sprints while developers implement current sprint
- QA validates completed stories while developers continue new ones
```

---

### Large Team (11-30 people)

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         LARGE TEAM (11-30)                               │
└─────────────────────────────────────────────────────────────────────────┘

Team Composition:
├─ 2-3 Product Owners (by product area)
├─ 1 Solution Architect
├─ 2-3 Technical Leads
├─ 15-20 Developers (3-4 scrum teams)
├─ 2-3 QA Engineers
├─ 1 Security Engineer
├─ 1 DevOps Engineer
└─ 2-3 Scrum Masters

ROLE DISTRIBUTION BY TEAM:

PRODUCT TEAM (Product Owners + Business Analysts):
├─ Daily: /speckit.specify (new features)
├─ Daily: /speckit.clarify (answer developer questions)
├─ Weekly: /speckit.checklist (UX and business rules)
└─ Maintains: Feature backlog with complete specs

ARCHITECTURE TEAM (Architect + Tech Leads):
├─ Weekly: /speckit.plan (all new features)
├─ Weekly: /speckit.tasks (all new features)
├─ Bi-weekly: /speckit.constitution (update standards)
├─ Before each sprint: /speckit.analyze (quality gate)
└─ Maintains: Technical design library

QUALITY TEAM (QA + Security + DevOps):
├─ Weekly: /speckit.checklist (domain-specific)
│  ├─ QA: Testing requirements
│  ├─ Security: Security requirements
│  └─ DevOps: Operational requirements
├─ Before sprints: /speckit.analyze (validate readiness)
└─ Maintains: Quality standards and checklists

DEVELOPMENT TEAMS (3-4 Scrum Teams):
├─ Scrum Team Alpha (5 developers + 1 Scrum Master)
├─ Scrum Team Beta (5 developers + 1 Scrum Master)
├─ Scrum Team Gamma (5 developers + 1 Scrum Master)
└─ Scrum Team Delta (5 developers + 1 Scrum Master)

Each Scrum Team:
├─ Daily: /speckit.implement (assigned stories)
├─ As needed: /speckit.clarify (when blocked)
└─ Maintains: Team velocity and task completion

COORDINATION WORKFLOW:

Week N-2 (Two weeks ahead):
├─ Product Owners: Specify features for Week N
│  └─ /speckit.specify + /speckit.clarify
│
└─ Quality Team: Generate checklists
   └─ /speckit.checklist (all domains)

Week N-1 (One week ahead):
├─ Architecture Team: Create designs
│  ├─ /speckit.plan
│  └─ /speckit.tasks
│
└─ All Teams: Review and approve
   └─ /speckit.analyze (quality gate)

Week N (Current sprint):
├─ Sprint Planning: All Scrum teams select stories
├─ Daily: Teams implement independently
│  └─ /speckit.implement
└─ Weekly: Cross-team sync for dependencies

PARALLEL EXECUTION:
- 4 Scrum teams can implement 4 independent features simultaneously
- Product team prepares Sprint N+2 while teams execute Sprint N
- Architecture team designs Sprint N+1 while teams execute Sprint N
- Quality team validates Sprint N-1 output while teams execute Sprint N

SCALING BENEFITS:
- Clear separation of concerns (product, architecture, quality, implementation)
- 2-sprint look-ahead ensures work is always ready
- Multiple teams can work independently on different features
- Consistent quality across all teams via shared constitution
```

---

### Enterprise Team (30+ people)

```
┌─────────────────────────────────────────────────────────────────────────┐
│                      ENTERPRISE TEAM (30+)                               │
└─────────────────────────────────────────────────────────────────────────┘

Team Composition:
├─ 5-10 Product Owners (by domain)
├─ 2-3 Solution Architects
├─ 5-8 Technical Leads
├─ 30-50 Developers (6-10 scrum teams)
├─ 5-8 QA Engineers
├─ 2-3 Security Engineers
├─ 2-3 DevOps Engineers
├─ 6-10 Scrum Masters
└─ 1-2 Engineering Managers

ORGANIZATIONAL STRUCTURE:

Business Unit A (e.g., User Management):
├─ 2 Product Owners
├─ 1 Solution Architect
├─ 2 Technical Leads
├─ 10-15 Developers (2-3 scrum teams)
├─ 2 QA Engineers
└─ 2-3 Scrum Masters

Business Unit B (e.g., Payment Processing):
├─ 2 Product Owners
├─ 1 Solution Architect
├─ 2 Technical Leads
├─ 10-15 Developers (2-3 scrum teams)
├─ 2 QA Engineers
└─ 2-3 Scrum Masters

Shared Services:
├─ Central Architecture Team (1 Principal Architect + 2 Architects)
├─ Central Quality Team (Security + DevOps + QA leadership)
└─ Engineering Management (sets standards via constitution)

SPEC KIT USAGE BY LAYER:

LAYER 1: Central Governance (One-time + Quarterly updates)
├─ Engineering Managers: /speckit.constitution
│  └─ Define enterprise-wide standards
│  └─ Updated quarterly or after major incidents
│
└─ Principal Architect: Reviews all /speckit.plan outputs
   └─ Ensures consistency across business units

LAYER 2: Business Unit Leadership (Weekly)
├─ Product Owners: /speckit.specify + /speckit.clarify
│  └─ 5-10 new specs per week across all units
│
├─ Solution Architects: /speckit.plan
│  └─ 5-10 new plans per week across all units
│
└─ Technical Leads: /speckit.tasks + /speckit.analyze
   └─ 5-10 new task breakdowns per week

LAYER 3: Quality Assurance (Continuous)
├─ QA Engineers: /speckit.checklist (testing)
├─ Security Engineers: /speckit.checklist (security)
└─ DevOps Engineers: /speckit.checklist (deployment)
   └─ 10-20 checklists per week across all units

LAYER 4: Development Teams (Daily)
└─ 6-10 Scrum Teams: /speckit.implement
   └─ 20-40 stories in flight at any given time

COORDINATION MECHANISMS:

1. Shared Constitution:
   - Single constitution.md for entire enterprise
   - All business units must comply
   - Changes require approval from all BU leaders

2. Design Review Board:
   - Weekly meeting of all Solution Architects
   - Reviews all /speckit.plan outputs
   - Ensures cross-BU consistency

3. Quality Council:
   - Bi-weekly meeting of QA, Security, DevOps leaders
   - Reviews /speckit.analyze reports
   - Identifies patterns and improvement opportunities

4. Specification Library:
   - Central repository of all spec.md files
   - Searchable by feature, domain, business unit
   - Prevents duplicate work across BUs

5. Metrics Dashboard:
   - Tracks Spec Kit usage across all teams
   - Monitors quality metrics (analyze results)
   - Identifies bottlenecks and optimization opportunities

SCALING CHALLENGES & SOLUTIONS:

Challenge 1: Inconsistent specifications across BUs
└─ Solution: Shared constitution + Design Review Board

Challenge 2: Quality gate bottlenecks
└─ Solution: Automated /speckit.analyze in CI/CD pipeline

Challenge 3: Cross-BU dependencies
└─ Solution: Central specification library + dependency tracking in tasks.md

Challenge 4: Knowledge silos
└─ Solution: Regular architecture knowledge sharing + shared plan.md templates

Challenge 5: Governance overhead
└─ Solution: Delegate most decisions to BUs, central team reviews exceptions only
```

---

## Project Type Adaptations

### Greenfield Project (New Product)

```
PHASE 0: Foundation (Week 1)
├─ Engineering Manager: /speckit.constitution
│  └─ Define all project principles upfront
│  └─ Duration: 2-4 hours
│
└─ Output: constitution.md (project bible)

PHASE 1: MVP Definition (Weeks 2-3)
├─ Product Owner: /speckit.specify (MVP features only)
│  └─ Focus on core value proposition
│  └─ 3-5 key features maximum
│
├─ Team: /speckit.checklist (all domains)
│  └─ Especially critical: Security and architecture
│
└─ Solution Architect: /speckit.plan
   └─ Make foundational technology decisions
   └─ Output: research.md with ALL key decisions documented

PHASE 2: Implementation (Weeks 4-12)
├─ Tech Lead: /speckit.tasks → /speckit.analyze
└─ Team: /speckit.implement (incremental MVP delivery)

BEST PRACTICES:
- Take time on constitution - it sets standards for entire project
- Document ALL technology decisions in research.md
- Start with minimal but complete features
- Focus on architecture that can scale
```

---

### Brownfield Project (Adding to Existing System)

```
PHASE 0: Assessment (Week 1)
├─ Review existing architecture and constraints
├─ Check if constitution.md exists
│  ├─ If yes: Review and update if needed
│  └─ If no: Create one: /speckit.constitution
│
└─ Document existing system in research.md

PHASE 1: Integration Planning (Week 2)
├─ Product Owner: /speckit.specify (new feature)
│  └─ Include: Integration points with existing system
│  └─ Include: Migration requirements if applicable
│
├─ Solution Architect: /speckit.plan
│  └─ Focus on: How new feature integrates with existing
│  └─ Document: Existing system constraints
│  └─ Create: Integration contracts in contracts/
│
└─ Output: plan.md with detailed integration strategy

PHASE 2: Task Breakdown (Week 2)
├─ Tech Lead: /speckit.tasks
│  └─ Include tasks for:
│     ├─ Existing code refactoring (if needed)
│     ├─ Database migrations
│     ├─ API versioning
│     └─ Backward compatibility
│
└─ Tech Lead: /speckit.analyze
   └─ Pay special attention to constitution compliance

PHASE 3: Implementation (Weeks 3-N)
└─ Developers: /speckit.implement
   └─ Extra care with existing code integration

BEST PRACTICES:
- Create constitution retrospectively if missing
- Document existing system constraints in plan.md
- Add integration tests to testing checklist
- Plan for rollback in deployment checklist
```

---

### Microservices Project

```
PATTERN: One Spec Kit workflow per microservice

SERVICE 1: User Service
├─ /speckit.specify → User management features
├─ /speckit.clarify
├─ /speckit.checklist (especially API contracts)
├─ /speckit.plan → Service-specific architecture
├─ /speckit.tasks → Service-specific tasks
├─ /speckit.analyze → Service-specific validation
└─ /speckit.implement

SERVICE 2: Payment Service
├─ [Same workflow]
└─ CRITICAL: contracts/ must define inter-service APIs

SERVICE 3: Notification Service
└─ [Same workflow]

CROSS-SERVICE COORDINATION:

Shared Constitution:
└─ ONE constitution.md for all services
   ├─ Defines: Service communication patterns
   ├─ Defines: Data ownership boundaries
   ├─ Defines: API versioning strategy
   └─ Defines: Observability requirements

Contract-First Development:
1. Define service contracts in contracts/ BEFORE implementation
2. Each service's contracts/ directory contains:
   ├─ OpenAPI specs for REST endpoints
   ├─ GraphQL schemas for GraphQL endpoints
   ├─ Event schemas for async communication
   └─ Message queue contracts

Service-Specific Specs:
├─ Each service has: specs/service-name/
│  ├─ spec.md (service-specific requirements)
│  ├─ plan.md (service-specific design)
│  ├─ data-model.md (service's own data)
│  ├─ contracts/ (service's API contracts)
│  └─ tasks.md (service-specific tasks)

VALIDATION STRATEGY:
└─ /speckit.analyze MUST validate:
   ├─ Contracts don't conflict across services
   ├─ Data ownership boundaries are respected
   └─ Inter-service dependencies are documented
```

---

### API-First Project

```
CRITICAL: contracts/ directory is PRIMARY artifact

PHASE 1: API Design (Before any other work)
├─ Product Owner: /speckit.specify (API requirements)
│  └─ Focus on: What operations API must support
│
└─ Solution Architect: /speckit.plan
   ├─ Create contracts/ FIRST (before implementation plan)
   │  ├─ Complete OpenAPI/GraphQL schemas
   │  ├─ All endpoints defined
   │  ├─ All request/response models defined
   │  └─ All error responses defined
   │
   └─ Then create plan.md (how to implement the contracts)

PHASE 2: Contract Validation
├─ Generate API checklist: /speckit.checklist (API focus)
│  └─ Validates:
│     ├─ All endpoints have error handling
│     ├─ All models have validation rules
│     ├─ API versioning strategy defined
│     └─ Authentication/authorization specified
│
└─ Team reviews and approves contracts/

PHASE 3: Parallel Development
├─ Backend Team: /speckit.implement (API implementation)
│  └─ Implements contracts exactly as specified
│
└─ Frontend Team: Can start immediately
   └─ Uses contracts/ to build against mock API

PHASE 4: Contract Testing
└─ QA: Tests implementation against contracts/
   └─ Every endpoint must match contract exactly

BEST PRACTICES:
- contracts/ is source of truth
- Never deviate from contracts without updating them
- Use contract testing tools (Pact, Dredd, etc.)
- Version APIs from day one
```

---

### Mobile App Project

```
PLATFORM-SPECIFIC WORKFLOWS:

BACKEND (API):
└─ Standard Spec Kit workflow
   └─ Focus on contracts/ (API specifications)

iOS APP:
├─ /speckit.specify (iOS-specific requirements)
│  └─ Include: iOS Human Interface Guidelines compliance
│  └─ Include: App Store requirements
│
├─ /speckit.checklist (iOS-specific)
│  └─ UX checklist: iOS design patterns
│  └─ Performance checklist: iOS performance requirements
│  └─ Accessibility checklist: VoiceOver, Dynamic Type
│
└─ /speckit.plan
   ├─ iOS-specific tech stack (SwiftUI vs. UIKit)
   └─ data-model.md: Core Data or other persistence

ANDROID APP:
├─ /speckit.specify (Android-specific requirements)
│  └─ Include: Material Design compliance
│  └─ Include: Google Play Store requirements
│
├─ /speckit.checklist (Android-specific)
│  └─ UX checklist: Material Design patterns
│  └─ Performance checklist: Android performance requirements
│  └─ Accessibility checklist: TalkBack, font scaling
│
└─ /speckit.plan
   ├─ Android-specific tech stack (Jetpack Compose vs. XML)
   └─ data-model.md: Room or other persistence

SHARED SPECIFICATIONS:
└─ constitution.md: Shared across all platforms
   ├─ API contracts must be identical
   ├─ Business logic must be consistent
   └─ Core user experience must be equivalent

COORDINATION:
└─ Product Owner creates ONE spec.md
   └─ Platform teams adapt to platform-specific plans
```

---

### Data Engineering Project

```
SPECIALIZED FOCUS: data-model.md is PRIMARY artifact

PHASE 1: Data Requirements
├─ Data Engineer: /speckit.specify
│  └─ Focus on: Data sources, transformations, destinations
│  └─ Focus on: Data quality requirements
│  └─ Focus on: Data volume and velocity
│
└─ Data Engineer: /speckit.clarify
   └─ Clarify: Data schemas, retention policies, privacy requirements

PHASE 2: Data Architecture
├─ Data Architect: /speckit.plan
│  └─ data-model.md is EXTENSIVE:
│     ├─ Source data schemas
│     ├─ Staging data schemas
│     ├─ Transform logic
│     ├─ Target data schemas
│     ├─ Data quality rules
│     └─ Data lineage documentation
│
└─ contracts/ contains:
   ├─ Data pipeline contracts
   ├─ API contracts for data access
   └─ Event schemas for data streaming

PHASE 3: Data Quality Checklist
└─ Data Engineer: /speckit.checklist (data quality focus)
   ├─ Data completeness requirements
   ├─ Data accuracy requirements
   ├─ Data consistency requirements
   ├─ Data timeliness requirements
   └─ Data governance requirements

PHASE 4: Pipeline Implementation
└─ Data Engineer: /speckit.implement
   └─ tasks.md includes:
      ├─ Data extraction tasks
      ├─ Data transformation tasks
      ├─ Data loading tasks
      ├─ Data quality validation tasks
      └─ Monitoring and alerting tasks

SPECIALIZED CHECKLISTS:
- Privacy/GDPR compliance checklist
- Data retention and deletion checklist
- Data security and encryption checklist
- Data pipeline monitoring checklist
```

---

## Quality Gates and Metrics

### Quality Gates by Phase

```
┌─────────────────────────────────────────────────────────────────────────┐
│                           QUALITY GATES                                  │
└─────────────────────────────────────────────────────────────────────────┘

GATE 1: Specification Quality
├─ Command: /speckit.specify + /speckit.clarify
├─ Criteria:
│  ├─ ✓ All mandatory sections present
│  ├─ ✓ 0 [NEEDS CLARIFICATION] markers remaining
│  ├─ ✓ All functional requirements are testable
│  ├─ ✓ All success criteria are measurable
│  ├─ ✓ requirements.md checklist 100% complete
│  └─ ✓ Product Owner approval
│
├─ Metrics:
│  ├─ Specification completeness: 100%
│  ├─ Average clarification rounds: Target ≤2
│  └─ Time to specification: Target ≤4 hours
│
└─ Decision: PASS → Proceed to validation | FAIL → Revise spec

─────────────────────────────────────────────────────────────────────────

GATE 2: Requirements Validation
├─ Command: /speckit.checklist (multi-domain)
├─ Criteria:
│  ├─ ✓ All domain checklists generated
│  │  └─ Minimum: UX, Testing, Security (if applicable)
│  ├─ ✓ All checklist items 100% complete
│  ├─ ✓ No critical issues flagged
│  └─ ✓ Multi-stakeholder approval
│
├─ Metrics:
│  ├─ Checklist coverage: 100% (all domains)
│  ├─ Critical issues: 0
│  └─ High issues: Target ≤3
│
└─ Decision: PASS → Proceed to design | FAIL → Revise requirements

─────────────────────────────────────────────────────────────────────────

GATE 3: Design Quality
├─ Command: /speckit.plan
├─ Criteria:
│  ├─ ✓ plan.md complete and approved
│  ├─ ✓ data-model.md complete (if applicable)
│  ├─ ✓ contracts/ complete (if applicable)
│  ├─ ✓ research.md documents all key decisions
│  ├─ ✓ Architecture checklist 100% complete
│  └─ ✓ Solution Architect approval
│
├─ Metrics:
│  ├─ Design completeness: 100%
│  ├─ Architecture review score: Target ≥8/10
│  └─ Technology decisions documented: 100%
│
└─ Decision: PASS → Proceed to task planning | FAIL → Revise design

─────────────────────────────────────────────────────────────────────────

GATE 4: Task Readiness
├─ Command: /speckit.tasks
├─ Criteria:
│  ├─ ✓ tasks.md complete with all tasks
│  ├─ ✓ All tasks have clear descriptions
│  ├─ ✓ All tasks have file paths specified
│  ├─ ✓ Dependencies identified and mapped
│  ├─ ✓ Parallel work opportunities identified
│  └─ ✓ Effort estimates provided
│
├─ Metrics:
│  ├─ Task clarity: 100% (no ambiguous tasks)
│  ├─ Average task size: Target 2-8 hours
│  └─ Parallel work ratio: Target ≥30%
│
└─ Decision: PASS → Proceed to analysis | FAIL → Refine tasks

─────────────────────────────────────────────────────────────────────────

GATE 5: Pre-Implementation Validation
├─ Command: /speckit.analyze
├─ Criteria:
│  ├─ ✓ Critical issues: 0
│  ├─ ✓ High issues: ≤3 (with mitigation plan)
│  ├─ ✓ Requirement coverage: ≥90%
│  ├─ ✓ Task coverage: ≥90%
│  ├─ ✓ No ambiguities detected
│  ├─ ✓ No conflicts detected
│  └─ ✓ Constitution compliance: 100%
│
├─ Metrics:
│  ├─ Requirement coverage: Target ≥90%, Threshold 100%
│  ├─ Task coverage: Target ≥90%, Threshold 100%
│  ├─ Constitution compliance: Must be 100%
│  ├─ Critical issues: Must be 0
│  └─ Ambiguity count: Target 0
│
└─ Decision: PASS → Proceed to implementation | FAIL → Fix issues

─────────────────────────────────────────────────────────────────────────

GATE 6: Implementation Quality (Standard CI/CD gates)
├─ Command: /speckit.implement (output validation)
├─ Criteria:
│  ├─ ✓ All tasks in tasks.md marked complete [X]
│  ├─ ✓ Code builds successfully
│  ├─ ✓ All unit tests pass
│  ├─ ✓ Code coverage ≥ constitution threshold
│  ├─ ✓ No critical code quality issues
│  ├─ ✓ Code review approved
│  └─ ✓ Matches plan.md architecture
│
├─ Metrics:
│  ├─ Task completion: 100%
│  ├─ Test pass rate: 100%
│  ├─ Code coverage: Per constitution (e.g., ≥80%)
│  ├─ Code quality: No critical issues
│  └─ Architecture compliance: 100%
│
└─ Decision: PASS → Proceed to testing | FAIL → Fix implementation

─────────────────────────────────────────────────────────────────────────

GATE 7: Acceptance
├─ Validation against: spec.md
├─ Criteria:
│  ├─ ✓ All functional requirements met
│  ├─ ✓ All acceptance criteria passed
│  ├─ ✓ All success criteria achieved
│  ├─ ✓ All integration tests pass
│  ├─ ✓ Performance meets requirements
│  ├─ ✓ Security scan clean
│  └─ ✓ Product Owner acceptance
│
├─ Metrics:
│  ├─ Requirements met: 100%
│  ├─ Acceptance criteria passed: 100%
│  ├─ Success criteria achieved: 100%
│  └─ Defect rate: Target <1 per 100 requirements
│
└─ Decision: PASS → Deploy | FAIL → Fix and retest
```

---

### Key Performance Indicators (KPIs)

```
┌─────────────────────────────────────────────────────────────────────────┐
│                     SPEC KIT KPIs & METRICS                              │
└─────────────────────────────────────────────────────────────────────────┘

SPEED METRICS (How fast are we moving?):

1. Specification Velocity
   ├─ Definition: Number of features specified per week
   ├─ Calculation: Count of complete spec.md files / week
   ├─ Target: 3-5 features/week (medium team)
   └─ Tracks: Product Owner efficiency with /speckit.specify

2. Design Velocity
   ├─ Definition: Number of features designed per week
   ├─ Calculation: Count of complete plan.md files / week
   ├─ Target: 3-5 features/week (medium team)
   └─ Tracks: Architect efficiency with /speckit.plan

3. Implementation Velocity
   ├─ Definition: Number of features implemented per sprint
   ├─ Calculation: Count of completed tasks.md (all tasks [X]) / sprint
   ├─ Target: Team-dependent (track and improve)
   └─ Tracks: Developer efficiency with /speckit.implement

4. Time to First Code
   ├─ Definition: Time from /speckit.specify to /speckit.implement start
   ├─ Calculation: Timestamp difference
   ├─ Target: ≤5 days (Agile), ≤2 weeks (Waterfall)
   └─ Tracks: Overall workflow efficiency

─────────────────────────────────────────────────────────────────────────

QUALITY METRICS (How good is our work?):

5. Specification Quality Score
   ├─ Definition: Percentage of specs passing first-time validation
   ├─ Calculation: (Specs with 0 revisions / Total specs) × 100
   ├─ Target: ≥80%
   └─ Tracks: /speckit.specify + /speckit.clarify effectiveness

6. Pre-Implementation Issue Rate
   ├─ Definition: Average issues found per /speckit.analyze
   ├─ Calculation: (Total issues / Total analyzes)
   ├─ Target: <5 issues, 0 critical
   └─ Tracks: Quality of specs, plans, and tasks

7. Requirement Coverage
   ├─ Definition: Percentage of requirements with tasks
   ├─ Calculation: From /speckit.analyze coverage report
   ├─ Target: 100%
   └─ Tracks: /speckit.tasks completeness

8. Post-Implementation Defect Rate
   ├─ Definition: Bugs found per feature after implementation
   ├─ Calculation: (Post-impl bugs / Features) × 100
   ├─ Target: <2 bugs per feature
   └─ Tracks: Overall workflow effectiveness

9. Rework Rate
   ├─ Definition: Percentage of features requiring spec changes post-planning
   ├─ Calculation: (Features with spec rework / Total features) × 100
   ├─ Target: <10%
   └─ Tracks: /speckit.clarify effectiveness

─────────────────────────────────────────────────────────────────────────

EFFICIENCY METRICS (Are we working smart?):

10. Clarification Efficiency
    ├─ Definition: Average clarification rounds per feature
    ├─ Calculation: Count of /speckit.clarify runs / feature
    ├─ Target: ≤2 rounds
    └─ Tracks: Initial spec quality

11. Checklist Completion Time
    ├─ Definition: Time from checklist creation to 100% complete
    ├─ Calculation: Timestamp difference
    ├─ Target: <2 days
    └─ Tracks: Validation process efficiency

12. Parallel Work Ratio
    ├─ Definition: Percentage of tasks that can run in parallel
    ├─ Calculation: (Tasks with [P] marker / Total tasks) × 100
    ├─ Target: ≥30%
    └─ Tracks: /speckit.tasks optimization

13. Constitution Compliance Rate
    ├─ Definition: Percentage of features passing constitution checks
    ├─ Calculation: From /speckit.analyze constitution section
    ├─ Target: 100%
    └─ Tracks: Team adherence to standards

14. Analysis Pass Rate
    ├─ Definition: Percentage of features passing /speckit.analyze first time
    ├─ Calculation: (Analyzes with 0 critical issues / Total analyzes) × 100
    ├─ Target: ≥90%
    └─ Tracks: Overall pre-implementation quality

─────────────────────────────────────────────────────────────────────────

COLLABORATION METRICS (Is the team working well together?):

15. Specification Review Cycle Time
    ├─ Definition: Time for stakeholders to complete checklists
    ├─ Calculation: From checklist creation to 100% complete
    ├─ Target: <3 days
    └─ Tracks: Stakeholder engagement

16. Cross-Functional Checklist Coverage
    ├─ Definition: Number of domains with checklists per feature
    ├─ Calculation: Count of checklist files per feature
    ├─ Target: ≥3 domains (UX, Testing, Security minimum)
    └─ Tracks: Multi-stakeholder involvement

17. Clarification Response Time
    ├─ Definition: Time from clarification question to answer
    ├─ Calculation: Time between /speckit.clarify calls
    ├─ Target: <4 hours
    └─ Tracks: Product Owner availability

─────────────────────────────────────────────────────────────────────────

BUSINESS METRICS (Are we delivering value?):

18. Feature Lead Time
    ├─ Definition: Time from /speckit.specify to production
    ├─ Calculation: End-to-end timestamp difference
    ├─ Target: <3 weeks (Agile), <3 months (Waterfall)
    └─ Tracks: Overall delivery efficiency

19. Specification Reuse Rate
    ├─ Definition: Percentage of specs reused/referenced in other features
    ├─ Calculation: (Reused specs / Total specs) × 100
    ├─ Target: Track trend (should increase over time)
    └─ Tracks: Pattern identification and reuse

20. Success Criteria Achievement Rate
    ├─ Definition: Percentage of spec.md success criteria achieved
    ├─ Calculation: (Achieved criteria / Total criteria) × 100
    ├─ Target: 100%
    └─ Tracks: Business value delivery
```

---

### Metrics Dashboard Example

```
┌─────────────────────────────────────────────────────────────────┐
│               SPEC KIT METRICS DASHBOARD                        │
│                      2025-10-26                                 │
└─────────────────────────────────────────────────────────────────┘

CURRENT SPRINT: Sprint 15 (Week 2 of 2)

SPEED METRICS:
├─ Specification Velocity: 4 features/week ✓ (Target: 3-5)
├─ Design Velocity: 4 features/week ✓ (Target: 3-5)
├─ Implementation Velocity: 3 features/sprint ⚠ (Target: 4)
└─ Time to First Code: 3.5 days ✓ (Target: ≤5 days)

QUALITY METRICS:
├─ Specification Quality Score: 85% ✓ (Target: ≥80%)
├─ Pre-Implementation Issue Rate: 3.2 issues ✓ (Target: <5)
├─ Requirement Coverage: 100% ✓ (Target: 100%)
├─ Post-Implementation Defect Rate: 1.5 bugs/feature ✓ (Target: <2)
└─ Rework Rate: 8% ✓ (Target: <10%)

EFFICIENCY METRICS:
├─ Clarification Efficiency: 1.8 rounds ✓ (Target: ≤2)
├─ Checklist Completion Time: 1.5 days ✓ (Target: <2 days)
├─ Parallel Work Ratio: 35% ✓ (Target: ≥30%)
├─ Constitution Compliance Rate: 100% ✓ (Target: 100%)
└─ Analysis Pass Rate: 92% ✓ (Target: ≥90%)

COLLABORATION METRICS:
├─ Specification Review Cycle Time: 2.2 days ✓ (Target: <3 days)
├─ Cross-Functional Checklist Coverage: 3.5 domains ✓ (Target: ≥3)
└─ Clarification Response Time: 3.1 hours ✓ (Target: <4 hours)

BUSINESS METRICS:
├─ Feature Lead Time: 2.5 weeks ✓ (Target: <3 weeks)
├─ Specification Reuse Rate: 15% (Improving ↑)
└─ Success Criteria Achievement Rate: 98% ⚠ (Target: 100%)

TRENDS (vs. Last Sprint):
├─ Speed: Stable →
├─ Quality: Improving ↑ (+5%)
├─ Efficiency: Improving ↑ (+8%)
└─ Collaboration: Stable →

ACTIONS REQUIRED:
1. ⚠ Implementation Velocity below target (3 vs. 4 features/sprint)
   → Action: Review task estimates, check for blockers
2. ⚠ Success Criteria Achievement Rate below 100%
   → Action: Review which criteria not met, refine definitions
```

---

## Best Practices and Patterns

### Best Practice 1: Specification-First Development

**DON'T:**
```
❌ Developer: "Let me just start coding and figure it out as I go"
   → Result: Scope creep, rework, missed requirements
```

**DO:**
```
✓ Product Owner: /speckit.specify → Clear requirements
✓ Team: /speckit.checklist → Validation
✓ Architect: /speckit.plan → Technical design
✓ Tech Lead: /speckit.tasks → Clear work breakdown
✓ Tech Lead: /speckit.analyze → Pre-implementation validation
✓ Developer: /speckit.implement → Confident implementation
   → Result: Clear direction, fewer defects, less rework
```

---

### Best Practice 2: Progressive Elaboration

**DON'T:**
```
❌ Try to specify everything perfectly upfront
   → Results in: Analysis paralysis, delayed delivery
```

**DO:**
```
✓ Sprint N-2: High-level specification
   /speckit.specify (just core requirements)

✓ Sprint N-1: Detailed elaboration
   /speckit.clarify + /speckit.checklist
   /speckit.plan + /speckit.tasks

✓ Sprint N: Implementation
   /speckit.implement

→ Result: Just-in-time elaboration, reduced waste
```

---

### Best Practice 3: Checklist-Driven Quality

**DON'T:**
```
❌ Skip checklists to "save time"
   → Result: Issues found late (expensive to fix)
```

**DO:**
```
✓ Create domain-specific checklists early
   - UX checklist (before design)
   - Security checklist (before design)
   - Testing checklist (before implementation)
   - Deployment checklist (before implementation)

✓ Make checklists mandatory quality gates
   → Result: Issues found early (cheap to fix)
```

---

### Best Practice 4: Constitution as Living Document

**DON'T:**
```
❌ Create constitution once and never update
   → Result: Standards drift, inconsistency
```

**DO:**
```
✓ Sprint Retrospective: Review constitution effectiveness
✓ After Major Issues: Update constitution with new principle
✓ Quarterly: Full constitution review and update
✓ /speckit.analyze: Enforces current constitution

→ Result: Standards evolve with team learning
```

---

### Best Practice 5: Parallel Work Optimization

**DON'T:**
```
❌ All tasks sequential (Developer A waits for Developer B)
   → Result: Low throughput, underutilized team
```

**DO:**
```
✓ /speckit.tasks generates parallel markers [P]
✓ Assign [P] tasks to different developers
✓ Use dependency graph to identify critical path
✓ Maximize parallel work within each user story

→ Result: Higher throughput, better team utilization
```

---

### Best Practice 6: Continuous Analysis

**DON'T:**
```
❌ Run /speckit.analyze once and forget
   → Result: Inconsistencies accumulate
```

**DO:**
```
✓ Before each sprint: /speckit.analyze (quality gate)
✓ After spec changes: /speckit.analyze (validate consistency)
✓ After design changes: /speckit.analyze (validate impact)
✓ CI/CD pipeline: Automated /speckit.analyze

→ Result: Continuous consistency validation
```

---

### Best Practice 7: Contract-Driven Development

**DON'T:**
```
❌ Define APIs informally ("We'll figure it out")
   → Result: Integration issues, rework
```

**DO:**
```
✓ /speckit.plan creates contracts/ with OpenAPI/GraphQL specs
✓ Frontend and backend develop against contracts independently
✓ Contract tests validate implementation matches contracts
✓ Contracts versioned and never changed (only extended)

→ Result: Parallel development, fewer integration issues
```

---

### Best Practice 8: User Story Driven Tasks

**DON'T:**
```
❌ Tasks organized by technical layer
   - All models first
   - All services next
   - All endpoints last
   → Result: No incremental value delivery
```

**DO:**
```
✓ /speckit.tasks organizes by user story
   - Story 1: Complete vertical slice (model + service + endpoint)
   - Story 2: Complete vertical slice
   → Result: Incremental value, early testing
```

---

### Best Practice 9: Traceability

**DON'T:**
```
❌ Lose connection between requirements and implementation
   → Result: Cannot verify all requirements met
```

**DO:**
```
✓ Every task in tasks.md has [US1], [US2] story label
✓ Every story in spec.md has unique ID
✓ /speckit.analyze validates traceability
✓ Checklists reference spec sections

→ Result: Complete traceability, coverage validation
```

---

### Best Practice 10: Early Feedback

**DON'T:**
```
❌ Wait until implementation complete to get feedback
   → Result: Major rework if direction wrong
```

**DO:**
```
✓ After /speckit.specify: Product Owner reviews spec.md
✓ After /speckit.clarify: Stakeholders review updated spec
✓ After /speckit.plan: Architect reviews plan.md
✓ After /speckit.tasks: Team reviews tasks breakdown
✓ After /speckit.analyze: Team reviews quality report

→ Result: Continuous feedback, early course correction
```

---

## Common Pitfalls and Solutions

### Pitfall 1: Specification Analysis Paralysis

**Problem:**
```
❌ Team spends weeks perfecting spec.md
❌ Every detail must be clarified before proceeding
❌ Product Owner overwhelmed with clarification questions
→ Result: Delayed delivery, no incremental progress
```

**Solution:**
```
✓ Timebox /speckit.specify to 4 hours maximum
✓ Limit /speckit.clarify to 5 questions maximum
✓ Use reasonable defaults, document assumptions
✓ Accept that some details will emerge during implementation
✓ Use /speckit.clarify during implementation for just-in-time answers

→ Result: Balanced speed and quality
```

---

### Pitfall 2: Checklist Fatigue

**Problem:**
```
❌ Team generates 10+ checklists per feature
❌ Each checklist has 50+ items
❌ Completing checklists takes weeks
❌ Team sees checklists as bureaucratic overhead
→ Result: Checklists abandoned, quality suffers
```

**Solution:**
```
✓ Generate only essential checklists (3-5 per feature)
✓ Limit checklist items to 20-30 per checklist
✓ Focus on high-impact quality checks only
✓ Make checklists specific to feature complexity
   - Simple feature: 1-2 checklists
   - Complex feature: 4-5 checklists
✓ Automate checklist items where possible

→ Result: Lightweight, valuable quality checks
```

---

### Pitfall 3: Over-Engineering in Planning

**Problem:**
```
❌ /speckit.plan generates overly complex architecture
❌ Solution designed for 10M users when current is 100
❌ Months spent on "future-proof" design
→ Result: Over-engineered, delayed, never used features
```

**Solution:**
```
✓ Plan for current needs + 1-2x growth
✓ Use constitution to define "appropriate complexity"
✓ Focus on extensibility, not premature optimization
✓ Validate plan.md complexity matches feature priority
   - P1 feature: Optimize for current needs
   - P3 feature: Simple implementation, refactor later if needed

→ Result: Appropriate design for actual needs
```

---

### Pitfall 4: Skipping Analysis

**Problem:**
```
❌ Team skips /speckit.analyze to "save time"
❌ Goes straight from /speckit.tasks to /speckit.implement
❌ Discovers requirement gaps during implementation
❌ Discovers design conflicts during implementation
→ Result: Mid-sprint rework, missed deadlines
```

**Solution:**
```
✓ Make /speckit.analyze mandatory quality gate
✓ Automate in CI/CD pipeline
✓ Block sprint start if analysis shows critical issues
✓ Track "analysis escape rate" (issues found post-analysis)
✓ Continuously improve analysis effectiveness

→ Result: Issues caught early, smoother implementation
```

---

### Pitfall 5: Ignoring Constitution Violations

**Problem:**
```
❌ /speckit.analyze shows constitution violations
❌ Team ignores them ("we'll fix it later")
❌ Technical debt accumulates
❌ Standards erode over time
→ Result: Inconsistent system, difficult to maintain
```

**Solution:**
```
✓ Make constitution compliance 100% mandatory
✓ /speckit.analyze must show 0 constitution violations to proceed
✓ If principle is wrong, update constitution (don't violate it)
✓ Track constitution violations as critical metric
✓ Review constitution effectiveness quarterly

→ Result: Consistent standards, maintainable system
```

---

### Pitfall 6: Poor Task Granularity

**Problem:**
```
❌ Tasks too large: "Implement user authentication" (40 hours)
❌ Tasks too small: "Add import statement" (5 minutes)
❌ Developer cannot track meaningful progress
→ Result: Poor velocity tracking, unclear progress
```

**Solution:**
```
✓ Target task size: 2-8 hours
✓ If task >8 hours, break down further in /speckit.tasks
✓ If task <1 hour, combine with related tasks
✓ Each task should produce visible, testable output
✓ Review task granularity in retrospectives

→ Result: Trackable progress, accurate velocity
```

---

### Pitfall 7: Stale Artifacts

**Problem:**
```
❌ spec.md updated but plan.md not updated
❌ plan.md changed but tasks.md not regenerated
❌ Artifacts drift out of sync
❌ /speckit.implement uses outdated information
→ Result: Implementation doesn't match current requirements
```

**Solution:**
```
✓ After updating spec.md: Re-run /speckit.clarify if needed
✓ After updating spec.md: Consider re-running /speckit.plan
✓ After updating plan.md: Always re-run /speckit.tasks
✓ After any update: Always re-run /speckit.analyze
✓ Use git hooks to enforce artifact consistency

→ Result: Artifacts stay in sync
```

---

### Pitfall 8: No Clear Ownership

**Problem:**
```
❌ No one responsible for maintaining spec.md
❌ No one responsible for keeping constitution.md updated
❌ Artifacts become outdated
→ Result: Loss of single source of truth
```

**Solution:**
```
✓ Product Owner owns spec.md (all changes go through them)
✓ Solution Architect owns plan.md and data-model.md
✓ Tech Lead owns tasks.md
✓ Engineering Manager owns constitution.md
✓ Document ownership in constitution.md
✓ Enforce through PR approval requirements

→ Result: Clear ownership, maintained artifacts
```

---

### Pitfall 9: Premature Implementation

**Problem:**
```
❌ Developer starts coding before /speckit.analyze passes
❌ "I'll just get a head start while waiting for approval"
❌ Requirements change after analysis
→ Result: Wasted effort, rework
```

**Solution:**
```
✓ Make /speckit.analyze passing a hard requirement to start coding
✓ Use feature flags to allow exploratory spikes (marked as such)
✓ Time-box spikes (4-8 hours maximum)
✓ Discard spike code, use learnings in proper implementation
✓ Track "premature implementation waste" metric

→ Result: Reduced waste, disciplined process
```

---

### Pitfall 10: Tool Over Process

**Problem:**
```
❌ Team focuses on using Spec Kit commands perfectly
❌ Forgets the underlying goal: delivering quality software
❌ Process becomes bureaucratic
→ Result: Compliance over value
```

**Solution:**
```
✓ Remember: Spec Kit is a tool to support your process, not the process itself
✓ Adapt commands to your context (skip what doesn't add value)
✓ Measure outcomes (quality, speed, satisfaction), not command usage
✓ Retrospective question: "Which Spec Kit commands helped us?"
✓ Evolve usage based on what works for your team

→ Result: Practical, value-driven usage
```

---

## Integration with Existing Tools

### Jira Integration

```
MAPPING SPEC KIT ARTIFACTS TO JIRA:

Epic:
└─ Created from: Multiple related spec.md files
   └─ Epic Description: Links to specs/ directory

User Story:
└─ Created from: spec.md user stories section
   ├─ Description: Copy from spec.md
   ├─ Acceptance Criteria: Copy from spec.md
   └─ Attachments:
      ├─ Link to spec.md
      ├─ Link to plan.md
      └─ Link to tasks.md

Subtasks:
└─ Created from: tasks.md task list
   ├─ Each task in tasks.md → One Jira subtask
   ├─ Task ID (T001) → Jira subtask ID
   └─ Checkbox status [ ] vs [X] → Jira status (To Do vs Done)

Custom Fields:
├─ "Specification URL" → Link to spec.md in git
├─ "Technical Plan URL" → Link to plan.md in git
├─ "Analysis Status" → /speckit.analyze pass/fail
└─ "Constitution Compliance" → Yes/No from analyze

AUTOMATION:
├─ Bot creates Jira issues from spec.md user stories
├─ Bot creates subtasks from tasks.md
├─ Bot updates status when tasks marked [X] in git
└─ Bot comments with /speckit.analyze results
```

---

### GitHub/GitLab Integration

```
REPOSITORY STRUCTURE:

repo/
├─ .github/
│  └─ workflows/
│     ├─ spec-kit-validation.yml (validate specs, plans, tasks)
│     └─ spec-kit-analysis.yml (run /speckit.analyze)
│
├─ memory/
│  └─ constitution.md (project governance)
│
├─ specs/
│  ├─ 001-user-auth/
│  │  ├─ spec.md
│  │  ├─ plan.md
│  │  ├─ data-model.md
│  │  ├─ tasks.md
│  │  ├─ contracts/
│  │  │  └─ auth-api.yaml
│  │  └─ checklists/
│  │     ├─ ux.md
│  │     ├─ security.md
│  │     └─ testing.md
│  │
│  └─ 002-payment-processing/
│     └─ [same structure]
│
└─ src/
   └─ [source code]

PULL REQUEST TEMPLATE:

## Feature: [Feature Name]

**Specification:** `specs/XXX-feature-name/spec.md`
**Technical Plan:** `specs/XXX-feature-name/plan.md`
**Tasks:** `specs/XXX-feature-name/tasks.md`

### Spec Kit Validation
- [ ] All checklists complete (100%)
- [ ] /speckit.analyze passed (0 critical issues)
- [ ] All tasks marked complete [X] in tasks.md
- [ ] Implementation matches plan.md architecture

### Acceptance Criteria
_Copy from spec.md_

### Testing
- [ ] Unit tests pass
- [ ] Integration tests pass
- [ ] Contract tests pass (against contracts/)

### Review Checklist
- [ ] Code review completed
- [ ] Security review completed (if applicable)
- [ ] Architecture review completed (if complex)
- [ ] Product Owner approval

BRANCH PROTECTION RULES:
├─ Require /speckit.analyze to pass (via CI)
├─ Require all checklists 100% complete
├─ Require PR approval from:
│  ├─ Code owner (Developer)
│  ├─ CODEOWNERS file references solution architect for plan.md changes
│  └─ Product Owner for spec.md changes
```

---

### Confluence/Documentation Integration

```
AUTOMATIC DOCUMENTATION GENERATION:

1. Feature Catalog Page:
   └─ Auto-generated from all spec.md files
      ├─ Table of features with status
      ├─ Links to specs/ directories
      └─ Updated on every commit

2. Architecture Decision Records (ADR):
   └─ Auto-generated from research.md files
      ├─ Each decision from research.md → One ADR
      └─ Format: Context, Decision, Rationale, Alternatives

3. API Documentation:
   └─ Auto-generated from contracts/
      ├─ OpenAPI specs → Swagger UI
      ├─ GraphQL schemas → GraphQL Playground
      └─ Hosted and updated automatically

4. Data Model Documentation:
   └─ Auto-generated from data-model.md
      ├─ Entity-Relationship Diagrams
      ├─ Table definitions
      └─ Relationship descriptions

CONFLUENCE STRUCTURE:

Project Home
├─ Constitution → Links to memory/constitution.md
├─ Feature Catalog → Auto-generated from specs/
├─ Architecture → Auto-generated from plan.md files
├─ API Reference → Auto-generated from contracts/
└─ Development Guide → Manual documentation
```

---

### Slack/Teams Integration

```
NOTIFICATIONS:

Bot: @speckit-bot

COMMANDS:
/speckit status <feature-name>
└─ Returns: Current status of all artifacts for feature

/speckit analyze <feature-name>
└─ Triggers: /speckit.analyze and posts results

/speckit help
└─ Shows: Quick reference of all commands

AUTOMATIC NOTIFICATIONS:

#product channel:
├─ New spec.md created → "@product-team New feature spec: [link]"
├─ Clarification needed → "@product-owner Clarification needed: [question]"
└─ Feature completed → "@product-team Feature [name] deployed"

#engineering channel:
├─ New plan.md created → "@architecture-team New design: [link]"
├─ Analysis failed → "@tech-lead Analysis failed with X critical issues"
└─ Tasks ready → "@dev-team New tasks ready for sprint"

#quality channel:
├─ New checklist created → "@quality-team Checklist needs review: [link]"
├─ Checklist completed → "@quality-team Checklist complete, feature ready"
└─ Constitution updated → "@all-team Constitution updated: [summary]"
```

---

### Monitoring/Observability Integration

```
METRICS COLLECTION:

Application Performance Monitoring (APM):
├─ Track: spec.md success criteria in production
│  └─ Example: "95% of requests complete in <200ms"
│  └─ Alert if not met
│
└─ Link production metrics back to original spec.md

Grafana Dashboard:
├─ Panel 1: Features delivered per sprint
│  └─ Source: Count of completed tasks.md
│
├─ Panel 2: Specification quality trends
│  └─ Source: /speckit.analyze metrics over time
│
├─ Panel 3: Requirements coverage
│  └─ Source: /speckit.analyze coverage reports
│
└─ Panel 4: Constitution compliance
   └─ Source: /speckit.analyze constitution checks

Error Tracking (Sentry/Rollbar):
└─ Tag errors with:
   ├─ Feature ID (from spec.md)
   ├─ User Story ID (from spec.md)
   └─ Task ID (from tasks.md)
   → Enables tracing production issues back to requirements
```

---

## Real-World Workflows

### Workflow 1: Bug Fix (Small Change)

```
TIME: 2-4 hours total

SCENARIO: Critical bug in production - payment processing fails over $1000

STEP 1: Specification (30 minutes)
├─ Product Owner: /speckit.specify
│  └─ Input: "Fix payment processing for amounts over $1000"
│  └─ Output: specs/099-fix-payment-limit/spec.md
│     ├─ Context: Bug description
│     ├─ Requirement: Support up to $10,000
│     ├─ Success Criteria: All test transactions succeed
│     └─ User Story: "As a user, I can process payments up to $10K"
│
└─ SKIP: /speckit.clarify (requirements clear)

STEP 2: Quick Validation (15 minutes)
└─ QA Engineer: /speckit.checklist (testing only)
   └─ Output: specs/099-fix-payment-limit/checklists/testing.md
      └─ 5 items: Edge cases to test

STEP 3: Technical Analysis (20 minutes)
└─ Tech Lead: /speckit.plan
   └─ Output: specs/099-fix-payment-limit/plan.md
      └─ Root cause: Stripe API limit configuration
      └─ Fix: Update API configuration

STEP 4: Task Breakdown (10 minutes)
└─ Tech Lead: /speckit.tasks
   └─ Output: specs/099-fix-payment-limit/tasks.md
      ├─ T001: Update Stripe API configuration
      ├─ T002: Update validation logic
      └─ T003: Add integration tests for $1K-$10K range

STEP 5: Quick Analysis (5 minutes)
└─ Tech Lead: /speckit.analyze
   └─ Result: 0 issues, proceed ✓

STEP 6: Implementation (1-2 hours)
├─ Developer: /speckit.implement
│  └─ Executes all 3 tasks
│
└─ Result: Bug fixed, tests pass

STEP 7: Deployment (30 minutes)
└─ DevOps: Deploy with monitoring

TOTAL OVERHEAD: ~1.5 hours of specification work for 1.5 hours of development
BENEFIT: Proper fix with tests, prevents regression, traceable
```

---

### Workflow 2: New Feature (Medium Complexity)

```
TIME: 2 weeks total (1 week prep, 1 week implementation)

SCENARIO: Add email notifications for order status changes

WEEK 1: PREPARATION

Monday (4 hours):
├─ Product Owner: /speckit.specify (1 hour)
│  └─ Defines: 4 notification types (order placed, shipped, delivered, cancelled)
│  └─ Output: specs/042-email-notifications/spec.md
│
├─ Product Owner: /speckit.clarify (30 minutes)
│  └─ AI asks 5 questions about template format, frequency, opt-out
│  └─ Answers documented in spec.md
│
└─ Business Analyst: /speckit.checklist (UX focus) (1 hour)
   └─ Output: specs/042-email-notifications/checklists/ux.md
      └─ 15 items covering email design, mobile responsiveness

Tuesday (4 hours):
├─ QA Engineer: /speckit.checklist (testing focus) (1 hour)
│  └─ Output: checklists/testing.md
│     └─ 20 items covering email delivery, edge cases
│
├─ Security Engineer: /speckit.checklist (security focus) (1 hour)
│  └─ Output: checklists/security.md
│     └─ 10 items covering data privacy, opt-out, GDPR
│
└─ DevOps Engineer: /speckit.checklist (deployment focus) (1 hour)
   └─ Output: checklists/deployment.md
      └─ 8 items covering email service setup, monitoring

Wednesday (2 hours):
└─ All stakeholders review and complete checklists
   └─ All checklists 100% complete by end of day

Thursday (4 hours):
├─ Solution Architect: /speckit.plan (2 hours)
│  └─ Output: specs/042-email-notifications/
│     ├─ research.md: Evaluated SendGrid, AWS SES, Mailgun → Choose SendGrid
│     ├─ plan.md: Email service architecture
│     ├─ data-model.md: EmailTemplate, EmailLog entities
│     └─ contracts/: Email API endpoints
│
└─ Solution Architect: /speckit.checklist (architecture) (1 hour)
   └─ Output: checklists/architecture.md
      └─ 12 items covering scalability, failure handling

Friday (2 hours):
├─ Tech Lead: /speckit.tasks (1 hour)
│  └─ Output: tasks.md with 18 tasks
│     ├─ Phase 1: Setup (2 tasks)
│     ├─ Phase 2: Foundational (3 tasks)
│     ├─ Phase 3: User Story 1 - Order Placed Email (4 tasks)
│     ├─ Phase 4: User Story 2 - Shipping Updates (4 tasks)
│     ├─ Phase 5: User Story 3 - Delivery Email (3 tasks)
│     └─ Phase 6: Polish (2 tasks)
│
└─ Tech Lead: /speckit.analyze (30 minutes)
   └─ Result: 0 critical issues, 2 medium issues
   └─ Medium issues resolved same day

WEEK 2: IMPLEMENTATION

Monday:
├─ Sprint Planning: Select stories 1-3 (all fit in sprint)
└─ Team starts: /speckit.implement

Tuesday-Thursday:
└─ Implementation continues (mark tasks complete in tasks.md)

Friday:
├─ Sprint Review: Demo email notifications
├─ Product Owner validates against spec.md acceptance criteria
└─ Feature accepted ✓

TOTAL EFFORT:
├─ Preparation: 22 hours across team
├─ Implementation: 60 hours (3 developers × 4 days × 5 hours coding)
└─ Overhead: 22/82 = 27% (prevents likely 30-40% rework without specs)
```

---

### Workflow 3: Large Initiative (Multi-Team, Multi-Month)

```
TIME: 4 months total

SCENARIO: Build entire e-commerce platform (MVP)

FEATURES:
1. User Authentication (15 story points)
2. Product Catalog (21 story points)
3. Shopping Cart (13 story points)
4. Checkout Flow (21 story points)
5. Order Management (13 story points)
6. Payment Processing (21 story points)
7. Email Notifications (8 story points)
8. Admin Dashboard (21 story points)

TOTAL: 133 story points, 3 teams, 4 months

MONTH 0: PROJECT SETUP

Week 1-2:
├─ Engineering Manager: /speckit.constitution (1 day)
│  └─ Defines: Testing standards, architecture principles, security requirements
│  └─ Output: memory/constitution.md
│
└─ Product Owners (3): /speckit.specify all 8 features (2 weeks)
   └─ Output: 8 complete spec.md files

Week 3:
└─ Product Owners: /speckit.clarify all features (20+ questions)
   └─ Output: All specs fully clarified

Week 4:
└─ All stakeholders: /speckit.checklist (multi-domain, all features)
   ├─ QA: 8 testing checklists
   ├─ Security: 8 security checklists
   ├─ DevOps: 8 deployment checklists
   └─ Architects: 8 architecture checklists
   └─ Result: 32 checklists, all 100% complete

MONTH 1: DESIGN & PLANNING

Week 5-6:
└─ Solution Architect: /speckit.plan for all 8 features (2 weeks)
   └─ Output: Complete technical design
      ├─ 8 × plan.md
      ├─ 1 × data-model.md (unified for all features)
      ├─ 8 × contracts/ directories
      └─ 1 × research.md (shared technology decisions)

Week 7:
└─ Tech Leads (3): /speckit.tasks for all features (1 week)
   └─ Output: 8 × tasks.md with 200+ total tasks

Week 8:
├─ Tech Leads: /speckit.analyze for all features (1 day)
│  └─ Result: 3 critical issues found and fixed
│  └─ Re-run analyze: 0 critical issues ✓
│
└─ Program Manager: Sprint planning for next 12 weeks
   └─ Assigns features to teams based on dependencies

MONTH 2-4: IMPLEMENTATION (12 weeks = 6 sprints)

TEAM ALLOCATION:
├─ Team Alpha: Features 1, 4, 7 (Authentication, Checkout, Notifications)
├─ Team Beta: Features 2, 5, 8 (Catalog, Orders, Admin)
└─ Team Gamma: Features 3, 6 (Cart, Payment)

SPRINT 1-2 (Foundation):
├─ All Teams: Setup and foundational work
├─ Team Alpha: Feature 1 (User Authentication)
└─ Team Beta: Feature 2 (Product Catalog)

SPRINT 3-4 (Core Features):
├─ Team Alpha: Feature 4 (Checkout Flow)
├─ Team Beta: Feature 5 (Order Management)
└─ Team Gamma: Features 3 + 6 (Cart + Payment)

SPRINT 5-6 (Polish):
├─ Team Alpha: Feature 7 (Email Notifications)
├─ Team Beta: Feature 8 (Admin Dashboard)
└─ All Teams: Integration testing, bug fixes

END OF MONTH 4:
├─ All 8 features complete
├─ All 133 story points delivered
├─ Platform deployed to production
└─ Success criteria from all spec.md files validated

METRICS:
├─ Total effort: 3 teams × 4 months × 5 people/team × 160 hours/month = 9,600 hours
├─ Spec Kit overhead: ~800 hours (8%)
├─ Rework avoided: Estimated 20-30% (1,900-2,900 hours)
└─ Net benefit: 1,100-2,100 hours saved (11-22%)
```

---

## Conclusion

GitHub Spec Kit provides a comprehensive, AI-augmented framework that maps to all phases of software development across multiple methodologies (Agile, Waterfall, Kanban) and team sizes (solo to enterprise).

### Key Success Factors:

1. **Adapt to Your Context**: Use what works, skip what doesn't add value
2. **Progressive Adoption**: Start with core commands, add more as needed
3. **Measure Outcomes**: Track quality, speed, and satisfaction, not just command usage
4. **Evolve Continuously**: Update constitution and practices based on retrospectives
5. **Automate Quality Gates**: Integrate into CI/CD for continuous validation
6. **Maintain Traceability**: Keep artifacts in sync from requirements to code
7. **Focus on Collaboration**: Use checklists to involve all stakeholders early
8. **Balance Speed and Quality**: Timebox specification work, accept iteration

### Recommended Starting Points by Team Type:

**Solo/Small Team (1-3 people):**
- Start with: specify → plan → implement
- Add later: clarify, tasks, analyze

**Medium Team (4-10 people):**
- Start with: specify → clarify → plan → tasks → implement
- Add later: checklist, analyze, constitution

**Large Team (11-30 people):**
- Start with: Full workflow (all 8 commands)
- Focus on: Multi-role checklists, mandatory quality gates

**Enterprise (30+ people):**
- Start with: Constitution first, then full workflow
- Focus on: Governance, consistency, automation

---

**Version**: 1.0
**Last Updated**: 2025-10-26
**Framework**: GitHub Spec Kit (SD2-METHOD)
